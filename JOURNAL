04/26/2014

I'm a little nervous about the UpdateSteamDataset process and when it updates playtime estimates, because it is not clearing out old data.  There are two potential ways to handle this.  

1.  We could set all playtime information to null prior to running the query.  
2.  We could keep track of the ones we updated, and run a second query that would set all that were not updated to null.

I like number two because if it fails then we still have data whereas if we fail during the update process for number one we don't have anything to fall back to because we wiped it all away.  I guess we could got to backups in order to get things back.

Speaking of backups, need to make sure that they are running...

04/25/2014

Ran out of time this morning.  

TODO: redeploy recommender in order to get the right DeployModel version.
TODO: schedule sample_training to run on Saturday.

04/24/2014

Jesus fucking christ maven is annoying.  I cannot get it to actually deploy a
release, but it's damn good at deploying snapshots that I don't give a shit
about.  I'm beginning to wonder if it is because of these warnings that appear
in the logs:

[INFO] [WARNING] 'build.plugins.plugin.version' for
org.apache.maven.plugins:maven-javadoc-plugin is missing.  
[INFO] [WARNING] 'build.plugins.plugin.version' for 
org.apache.maven.plugins:maven-deploy-plugin
is missing.  
[INFO] [WARNING] 'build.plugins.plugin.version' for
org.apache.maven.plugins:maven-source-plugin is missing.

According to Google, those shouldn't really be causing the problems that I've
been encountering.  Of course not, why would that do what I think it should do.

Definitely not a productive morning.  Didn't do any real work and instead
fought with maven build files and wasted my fucking time.  Frustrated!

TODO: Was running the sample_training job and it was failing because
CollaborativeFilter expected that we would have steamId <tab> JSON.  Determine
what the correct way is and make it work tomorrow morning.

04/23/2014

Thursday: make sure to get training ready for Friday.  We are running low on time.

Deployed ml-0.0.1 so now we can add it as a dependency which should make things 
a bit easier for us going forward.  Checked in the latest versions of 
TrainingData and Deploy so that we are up to date there.

I had a thought while running this morning that may help us with the problem 
where we have a new user that wasn't part of the training data, yet they have 
lots of training data at run time.  I already figured that I could use SGD to 
tease out the preference at run time, but that could potentially be slow and if 
a user repeatedly visits, they could skew the results.

My second thought is to offline do a PCA over the user x games matrix.  If I 
reduce it to around 100 predictor variables then I could take a new user, run 
them through the PCA in order to get their lower dimension representation and 
from there have a trained MF model that I would not need to update, but could 
extract recommendations from.  Needs more fleshing out, but so far, not a 
terrible idea.

TODO: There is still a bug where when I deploy using mvn:release sometimes I get 
snapshot versions and sometimes I do not.  I need to understand what causes 
that.  It could be that I just need to separate out the development repos from 
the release repos. 

04/22/2014

Today I started setting up the sampled data.  I had to create a new job that 
would pull the training data out of the MySQL database into a local file that we 
could write to.  

TODO: TrainingData and Deploy model have local changes that still need to be 
deployed.  These are the new options that take input and output files so that I 
don't need to hardcode paths anymore.

TODO: Need to deploy ml.jar which will contain domain independent ML code.  I'm 
starting to separate out funcationality to make reuse easier.

TODO: UpdateSteamDataset is probably broken in a lot of places.  Follow up to 
make sure that it works everywhere and things are still functioning properly.

Once these guys are all in place, then I can get back to the real task of trying 
out different algorithms.  Still not certain how to do matrix factorization when 
you have a new matrix at run time ~ i.e. you haven't seen the user in the 
training data.
