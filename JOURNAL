05/29/2014

Today I was able to finish up the changes for searching for steam id from the interface.  These changes have been pushed into production and will allow users to enter a community name and allow us to find them.  Occasionally it will screw up if people believe that their login name == their community name which is not true.

Next week we can start working on improving the recommendations again and see what algorithms are worth implementing / playing around with.

05/28/2014

Another good day.  Today I've made significant process towards simplifying the login / retrieval of a player's profile.  I found two different API calls that make my life much easier.  The first will resolve a URL given a vanity name (deerslyr1) and will return the steam id of the player.  The second is a player summary that gives me all of the details like avatar, name, login status, etc.  The wiring is mostly done and all that really remains is to create a faux header that contains all of this information on the profile page and the recommendation page.  This is so that you always know whose recommendations you are receiving.  Once this is fully working we can go back to making the blogger entries and trying to get people to show up to the site.

05/23/2014

Been a long time since I updated my journal, probably because I haven't been able to had the desire to work on steamrecommender for a while now.  This morning I worked on resolving vanity urls with the given steam api endpoint.  The backend was easy enough to write (not checked in) and now I just need to get the front end user interface wired up and looking pretty before I can ship out this incremental change.

I lost some details for one of my tasks in Google tasks.  May think about how this will affect how much I trust google tasks going forward.  Definitely also need to spend some time every day thinking about what I'm going to build next.

04/26/2014

I'm a little nervous about the UpdateSteamDataset process and when it updates
playtime estimates, because it is not clearing out old data.  There are two
potential ways to handle this.  

1.  We could set all playtime information to null prior to running the query.
2.  We could keep track of the ones we updated, and run a second query that
would set all that were not updated to null.

I like number two because if it fails then we still have data whereas if we
fail during the update process for number one we don't have anything to fall
back to because we wiped it all away.  I guess we could got to backups in order
to get things back.

Speaking of backups, need to make sure that they are running...

04/25/2014

Ran out of time this morning.  

TODO: redeploy recommender in order to get the right DeployModel version.
TODO: schedule sample_training to run on Saturday.

04/24/2014

Jesus fucking christ maven is annoying.  I cannot get it to actually deploy a
release, but it's damn good at deploying snapshots that I don't give a shit
about.  I'm beginning to wonder if it is because of these warnings that appear
in the logs:

[INFO] [WARNING] 'build.plugins.plugin.version' for
org.apache.maven.plugins:maven-javadoc-plugin is missing.  
[INFO] [WARNING] 'build.plugins.plugin.version' for 
org.apache.maven.plugins:maven-deploy-plugin
is missing.  
[INFO] [WARNING] 'build.plugins.plugin.version' for
org.apache.maven.plugins:maven-source-plugin is missing.

According to Google, those shouldn't really be causing the problems that I've
been encountering.  Of course not, why would that do what I think it should do.

Definitely not a productive morning.  Didn't do any real work and instead
fought with maven build files and wasted my fucking time.  Frustrated!

TODO: Was running the sample_training job and it was failing because
CollaborativeFilter expected that we would have steamId <tab> JSON.  Determine
what the correct way is and make it work tomorrow morning.

04/23/2014

Thursday: make sure to get training ready for Friday.  We are running low on time.

Deployed ml-0.0.1 so now we can add it as a dependency which should make things 
a bit easier for us going forward.  Checked in the latest versions of 
TrainingData and Deploy so that we are up to date there.

I had a thought while running this morning that may help us with the problem 
where we have a new user that wasn't part of the training data, yet they have 
lots of training data at run time.  I already figured that I could use SGD to 
tease out the preference at run time, but that could potentially be slow and if 
a user repeatedly visits, they could skew the results.

My second thought is to offline do a PCA over the user x games matrix.  If I 
reduce it to around 100 predictor variables then I could take a new user, run 
them through the PCA in order to get their lower dimension representation and 
from there have a trained MF model that I would not need to update, but could 
extract recommendations from.  Needs more fleshing out, but so far, not a 
terrible idea.

TODO: There is still a bug where when I deploy using mvn:release sometimes I get 
snapshot versions and sometimes I do not.  I need to understand what causes 
that.  It could be that I just need to separate out the development repos from 
the release repos. 

04/22/2014

Today I started setting up the sampled data.  I had to create a new job that 
would pull the training data out of the MySQL database into a local file that we 
could write to.  

TODO: TrainingData and Deploy model have local changes that still need to be 
deployed.  These are the new options that take input and output files so that I 
don't need to hardcode paths anymore.

TODO: Need to deploy ml.jar which will contain domain independent ML code.  I'm 
starting to separate out functionality to make reuse easier.

TODO: UpdateSteamDataset is probably broken in a lot of places.  Follow up to 
make sure that it works everywhere and things are still functioning properly.

Once these guys are all in place, then I can get back to the real task of trying 
out different algorithms.  Still not certain how to do matrix factorization when 
you have a new matrix at run time ~ i.e. you haven't seen the user in the 
training data.
